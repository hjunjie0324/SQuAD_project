# -*- coding: utf-8 -*-
"""setup2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0nep7aaqkgO5m8UpbMtk1Tq23_EaG96
"""

import numpy as np
import pandas as pd
import json

import requests
import urllib

import re
import string

def load_data(train_df):
    contexts = []
    questions = []
    answers = []
    ids = []
    for i in range(train_df['data'].shape[0]):
        topic = train_df['data'].iloc[i]['paragraphs']
        for sub_para in topic:
            context = sub_para['context']
            for q_a in sub_para['qas']:
                qid = q_a['id']
                question = q_a['question']
                for answer in q_a['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)
                    ids.append(qid)

    return contexts, questions, answers, ids

val_response = urllib.request.urlopen("https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json")
val_raw = pd.read_json(val_response)

val_contexts, val_questions, val_answers, val_ids = load_data(val_raw)


def add_end_idx(answers,contexts):
  for answer, context in zip(answers, contexts):
    gold_text = answer['text']
    start_idx = answer['answer_start']
    end_idx = start_idx + len(gold_text)

    #sometimes SQuAD answers are off by a character or two 
    if context[start_idx:end_idx] == gold_text:
      answer['answer_end'] = end_idx
    elif context[start_idx-1:end_idx-1] == gold_text:
      answer['answer_start'] = start_idx - 1
      answer['answer_end'] = end_idx - 1
    elif context[start_idx-2: end_idx-2] == gold_text:
      answer['answer_start'] = start_idx - 2
      answer['answer_end'] = end_idx - 2

add_end_idx(val_answers, val_contexts)

from transformers import BertTokenizer, BertForQuestionAnswering, BertTokenizerFast
import torch

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

small_val_contexts = val_contexts
small_val_questions = val_questions
small_val_answers = val_answers
small_val_ids = val_ids

small_val_encodings = tokenizer(small_val_contexts, small_val_questions, truncation=True, padding=True)

#start and end position in SQuAD dataset is character position. change it to token position below
def add_token_positions(encodings, answers):
  start_positions = []
  end_positions = []
  for i in range(len(answers)):
    start_positions.append(encodings.char_to_token(i,answers[i]['answer_start']))
    end_positions.append(encodings.char_to_token(i,answers[i]['answer_end']-1))
    if start_positions[-1] is None:
      start_positions[-1] = tokenizer.model_max_length
    if end_positions[-1] is None:
      end_positions[-1] = tokenizer.model_max_length
  encodings.update({'start_positions':start_positions,'end_positions':end_positions})

add_token_positions(small_val_encodings,small_val_answers)

class SquadDataset(torch.utils.data.Dataset):
  def __init__(self,encodings):
    self.encodings = encodings
  def __getitem__(self,idx):
    return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}
  def __len__(self):
    return len(self.encodings.input_ids)

small_val_dataset = SquadDataset(small_val_encodings)

#from torch.utils.data import DataLoader
#from transformers import AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased",return_dict=True)

model.to(device)
model.train()

#train_loader = DataLoader(small_train_dataset,batch_size=16,shuffle=True)

#optim = AdamW(model.parameters(),lr=5e-5)

model.load_state_dict(torch.load("baseModel.pt"))
model.eval()

prediction_dict = {}

def normalize_answer(s):
  """Lower text and remove punctuation, articles and extra whitespace."""
  def remove_articles(text):
    regex = re.compile(r'\b(a|an|the)\b', re.UNICODE)
    return re.sub(regex, ' ', text)
  def white_space_fix(text):
    return ' '.join(text.split())
  def remove_punc(text):
    exclude = set(string.punctuation)
    return ''.join(ch for ch in text if ch not in exclude)
  def lower(text):
    return text.lower()
  return white_space_fix(remove_articles(remove_punc(lower(s))))

def get_tokens(s):
  if not s: return []
  return normalize_answer(s).split()

def convert_token_to_prediction(tokens, start, end):
    prediction = tokens[start]
    for i in range(start+1, end+1):
        if tokens[i][0:2] == '##':
            prediction += tokens[i][2:]
        else:
            prediction += ' ' + tokens[i]
    return prediction

import collections

"""
nlp = spacy.blank("en")

def word_tokenize(sent):
    doc = nlp(sent)
    return [token.text for token in doc]
"""

def get_F1_score(golden_answer, prediction):
    golden_tokens = get_tokens(golden_answer)
    pred_tokens = get_tokens(prediction)
    
    common = collections.Counter(golden_tokens) & collections.Counter(pred_tokens)
    num_same = sum(common.values())

    if len(pred_tokens) == 0 or len(golden_tokens) == 0:  #ignore the case of no-answer at this stage
        return 0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(golden_tokens)
    if precision + recall == 0:
        f1 = 0
    else:
        f1 = (2 * precision * recall) / (precision + recall)
    return f1

def predict(eval_dataset,qids,answers):
    n = len(eval_dataset)
    exact_match = 0
    f1_sum = 0
    for i in range(n):
        input_ids = eval_dataset[i]['input_ids'].to(device)
        attention_mask = eval_dataset[i]['attention_mask'].to(device)
        qid = small_val_ids[i]

        golden_answer = answers[i]['text']

        with torch.no_grad():
            output = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))
        start = torch.argmax(output[0][0])
        end = torch.argmax(output[1][0])
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        prediction = convert_token_to_prediction(tokens, start, end)
        #prediction = ' '.join(tokens[start:end+1])
        prediction_dict[qid] = prediction

        #exact match
        if(normalize_answer(prediction) == normalize_answer(golden_answer)):
            exact_match = exact_match + 1
        #F1_score
        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)

        """
        print("qid:",qid)
        print("prediction:",prediction)
        print("...............")
        """
    accuracy = exact_match / n
    f1 = f1_sum / n
    return accuracy, f1

accuracy, f1 = predict(small_val_dataset,small_val_ids, small_val_answers)
print("accuracy:",accuracy)
print("f1",f1)

with open('prediction.txt','w') as filehandle:
    filehandle.write(json.dumps(prediction_dict))

"""


def evaluate(eval_dataset, answers):
    n = len(eval_dataset)
    exact_match = 0
    f1_sum = 0
    for i in range(n):
        input_ids = eval_dataset[i]['input_ids'].to(device)
        attention_mask = eval_dataset[i]['attention_mask'].to(device)
        golden_answer = answers[i]['text']
        with torch.no_grad():
            output = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))
        start = torch.argmax(output[0][0])
        end = torch.argmax(output[1][0])
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        prediction = ' '.join(tokens[start:end+1])
        print("prediction:", prediction)
        print("golden_answer:", golden_answer)

        #exact match
        if(prediction == golden_answer):
            exact_match = exact_match + 1

        #F1_score
        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)
        
        
    accuracy = exact_match/n
    f1 = f1_sum / n
    return accuracy, f1

accuracy, f1 = evaluate(small_val_dataset, small_val_answers)

print("...................")
print("accuracy:",accuracy)
print("f1",f1)
"""

