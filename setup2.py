# -*- coding: utf-8 -*-
"""setup2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0nep7aaqkgO5m8UpbMtk1Tq23_EaG96
"""

import numpy as np
import pandas as pd
import json
import spacy


import requests
import urllib

def load_data(train_df):
    contexts = []
    questions = []
    answers = []
    for i in range(train_df['data'].shape[0]):
        topic = train_df['data'].iloc[i]['paragraphs']
        for sub_para in topic:
            context = sub_para['context']
            for q_a in sub_para['qas']:
                question = q_a['question']
                for answer in q_a['answers']:
                    contexts.append(context)
                    questions.append(question)
                    answers.append(answer)

    return contexts, questions, answers



train_response = urllib.request.urlopen("https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json")
train_raw = pd.read_json(train_response)

val_response = urllib.request.urlopen("https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json")
val_raw = pd.read_json(val_response)

train_contexts, train_questions, train_answers = load_data(train_raw)
val_contexts, val_questions, val_answers = load_data(val_raw)


def add_end_idx(answers,contexts):
  for answer, context in zip(answers, contexts):
    gold_text = answer['text']
    start_idx = answer['answer_start']
    end_idx = start_idx + len(gold_text)

    #sometimes SQuAD answers are off by a character or two 
    if context[start_idx:end_idx] == gold_text:
      answer['answer_end'] = end_idx
    elif context[start_idx-1:end_idx-1] == gold_text:
      answer['answer_start'] = start_idx - 1
      answer['answer_end'] = end_idx - 1
    elif context[start_idx-2: end_idx-2] == gold_text:
      answer['answer_start'] = start_idx - 2
      answer['answer_end'] = end_idx - 2

add_end_idx(train_answers, train_contexts)
add_end_idx(val_answers, val_contexts)

from transformers import BertTokenizer, BertForQuestionAnswering, BertTokenizerFast
import torch

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

small_train_contexts = train_contexts
small_train_questions = train_questions
small_train_answers = train_answers

small_val_contexts = val_contexts[0:1000]
small_val_questions = val_questions[0:1000]
small_val_answers = val_answers[0:1000]

small_train_encodings = tokenizer(small_train_contexts, small_train_questions, truncation=True, padding=True)
small_val_encodings = tokenizer(small_val_contexts, small_val_questions, truncation=True, padding=True)

#start and end position in SQuAD dataset is character position. change it to token position below
def add_token_positions(encodings, answers):
  start_positions = []
  end_positions = []
  for i in range(len(answers)):
    start_positions.append(encodings.char_to_token(i,answers[i]['answer_start']))
    end_positions.append(encodings.char_to_token(i,answers[i]['answer_end']-1))
    if start_positions[-1] is None:
      start_positions[-1] = tokenizer.model_max_length
    if end_positions[-1] is None:
      end_positions[-1] = tokenizer.model_max_length
  encodings.update({'start_positions':start_positions,'end_positions':end_positions})

add_token_positions(small_train_encodings,small_train_answers)
add_token_positions(small_val_encodings,small_val_answers)

class SquadDataset(torch.utils.data.Dataset):
  def __init__(self,encodings):
    self.encodings = encodings
  def __getitem__(self,idx):
    return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}
  def __len__(self):
    return len(self.encodings.input_ids)

small_train_dataset = SquadDataset(small_train_encodings)
small_val_dataset = SquadDataset(small_val_encodings)

from torch.utils.data import DataLoader
from transformers import AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = BertForQuestionAnswering.from_pretrained("bert-base-uncased",return_dict=True)

model.to(device)
model.train()

train_loader = DataLoader(small_train_dataset,batch_size=16,shuffle=True)

optim = AdamW(model.parameters(),lr=5e-5)

#train_process
curr_loss = 0
all_loss = []
iteration = 0
plot_every = 500
log_every = 100
for epoch in range(3):
  for batch in train_loader:
    optim.zero_grad()
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    start_positions = batch['start_positions'].to(device)
    end_positions = batch['end_positions'].to(device)
    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions,
                    end_positions=end_positions)
    loss = outputs[0]
    curr_loss += curr_loss + loss.item()
    iteration += 1
    if iteration % plot_every == 0:
        all_loss.append(curr_loss/plot_every)
        curr_loss = 0
    if iteration % log_every == 0:
        print("iteration:",iteration," loss",loss.item())
    loss.backward()
    optim.step()

model.eval()

torch.save(model.state_dict(),'baseModel.pt')
nlp = spacy.blank("en")

def word_tokenize(sent):
    doc = nlp(sent)
    return [token.text for token in doc]

import collections

def get_F1_score(golden_answer, prediction):
    golden_tokens = word_tokenize(golden_answer)
    pred_tokens = word_tokenize(prediction)
    
    common = collections.Counter(golden_tokens) & collections.Counter(pred_tokens)
    num_same = sum(common.values())

    if len(pred_tokens) == 0 or len(golden_tokens) == 0:  #ignore the case of no-answer at this stage
        return 0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(golden_tokens)
    if precision + recall == 0:
        f1 = 0
    else:
        f1 = (2 * precision * recall) / (precision + recall)
    return f1

def evaluate(eval_dataset, answers):
    n = len(eval_dataset)
    exact_match = 0
    f1_sum = 0
    for i in range(n):
        input_ids = eval_dataset[i]['input_ids'].to(device)
        attention_mask = eval_dataset[i]['attention_mask'].to(device)
        golden_answer = answers[i]['text']
        with torch.no_grad():
            output = model(torch.unsqueeze(input_ids,0), torch.unsqueeze(attention_mask,0))
        start = torch.argmax(output[0][0])
        end = torch.argmax(output[1][0])
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
        prediction = ' '.join(tokens[start:end+1])

        #exact match
        if(prediction == golden_answer):
            exact_match = exact_match + 1

        #F1_score
        f1_sum = f1_sum + get_F1_score(golden_answer, prediction)
        
        
    accuracy = exact_match/n
    f1 = f1_sum / n
    return accuracy, f1

accuracy, f1 = evaluate(small_val_dataset, small_val_answers)

print("...................")
print("accuracy:",accuracy)
print("f1",f1)

with open('loss.txt', 'w') as filehandle:
    for loss in all_loss:
      filehandle.write('%s\n' % loss)

